{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whatsapp Scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm scrapes images, messages and forwarded messages from whatsapp groups and applies image identification, toxic text dectection and news similarity check algorithm respectively."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To run this code, you will need to have an account on Whatsapp. This account shall be used for monitoring purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "filename_1 = 'TextModel_SVM.sav'\n",
    "filename_2 = 'Vocabulary.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sakshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sakshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from pandas import DataFrame as df\n",
    "nltk.download('stopwords')\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pickle\n",
    "import anvil\n",
    "import anvil.media\n",
    "import cv2\n",
    "from pytesseract import *\n",
    "#Paste your path to tesseract.exe below\n",
    "pytesseract.tesseract_cmd= r'C:\\Users\\Sakshi\\AppData\\Local\\Tesseract-OCR\\tesseract.exe'\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initlialising Toxic Text Detection Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_toxic = pd.read_csv('Hindi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(input_data, data_type):\n",
    "    if data_type == 'image':\n",
    "        result = image_processing(input_data, model1)\n",
    "        return result\n",
    "    \n",
    "    if data_type == 'news':\n",
    "        result = news_processing(input_data)\n",
    "        return result\n",
    "        \n",
    "    if data_type == 'personalviews' or data_type == 'comment':\n",
    "        model = pickle.load(open(filename_1, 'rb'))\n",
    "        vect =  pickle.load(open(filename_2, \"rb\"))\n",
    "        result = comment_processing(input_data, model, vect)\n",
    "        return result\n",
    "    if data_type == 'newsimage':\n",
    "        result = newsimage_processing(input_data)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_processing(input_data, clf, loaded_vec):\n",
    "    loaded_vec = CountVectorizer(decode_error=\"replace\",vocabulary=loaded_vec)\n",
    "    text = []\n",
    "    text.append(input_data)\n",
    "    score = 0\n",
    "    keyword = re.sub('[^a-zA-Z]',' ',text[0])\n",
    "    keyword = keyword.lower()\n",
    "    for i in keyword.split(' '):\n",
    "        for j in hindi_toxic['Words']:\n",
    "            if j == i:\n",
    "                score = 1\n",
    "                break\n",
    "    if score == 1:\n",
    "        print('Score: ', score)\n",
    "        return('Not Okay')\n",
    "    else:\n",
    "        X = loaded_vec.transform(text)\n",
    "        score = clf.predict(X)\n",
    "        print('Score', score[0])\n",
    "        if score == 1:\n",
    "            return('Not Okay')\n",
    "        else:\n",
    "            return('Okay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialising Image Classification Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_processing(path):\n",
    "    model = pickle.load(open(r'finalized_model.sav', 'rb'))\n",
    "    image1 = ['jpg', 'jpeg', 'png','jfif']\n",
    "    video1 = ['mp4', 'gif']\n",
    "    result = 0\n",
    "    #Enter your path to an image\n",
    "    path = path\n",
    "    extension = path.split('.')[-1]\n",
    "    print(extension)\n",
    "    if extension in image1:\n",
    "        print('yes')\n",
    "        test_img = keras.preprocessing.image.load_img(path, target_size = (32,32))\n",
    "        print(type(test_img))\n",
    "        test_img = tf.keras.preprocessing.image.img_to_array(test_img)\n",
    "        print(type(test_img))\n",
    "        test_img = np.expand_dims(test_img, axis = 0)\n",
    "        result = model.predict(test_img)\n",
    "        print(result)\n",
    "        if result[0][4] == 1 or result[0][5] == 1 or [0][0] == 1:\n",
    "            print('Result: Okay')\n",
    "            return ('Okay')\n",
    "        else:\n",
    "            print('Result: Not Okay') \n",
    "            return('Not Okay')\n",
    "\n",
    "    if extension in video1:\n",
    "        res = []\n",
    "        vidcap = cv2.VideoCapture(path)\n",
    "        success, image = vidcap.read()\n",
    "        count = 0\n",
    "        while success:\n",
    "            success,image = vidcap.read()\n",
    "            if success:\n",
    "                print('read a new frame:', success)\n",
    "                count+=1\n",
    "                images = cv2.resize(image, (32,32),3)\n",
    "                #images  = images.reshape(32,32,3)\n",
    "                print(image.shape)\n",
    "                test_img = np.expand_dims(images, axis = 0)\n",
    "                result = model.predict(test_img)\n",
    "                res.append(result)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        print(len(res))\n",
    "        okay = 0\n",
    "        notokay = 0\n",
    "        for result in res:\n",
    "            print(res)\n",
    "            if result[0][4] == 1 or result[0][5] == 1 or [0][0] == 1:\n",
    "                okay+=1 \n",
    "            else:\n",
    "                notokay+=1\n",
    "        print(okay)\n",
    "        print(notokay)\n",
    "        if okay > notokay:\n",
    "            print('Result: Okay')\n",
    "            return('Okay')\n",
    "        else:\n",
    "            print('Result: Not Okay')\n",
    "            return('Not Okay')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extracting Text from Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagetext_processing(path):\n",
    "        img = keras.preprocessing.image.load_img(path)\n",
    "        try:\n",
    "            text=pytesseract.image_to_string(img)\n",
    "            print('Text', text)\n",
    "            score = processing(text,'comment')\n",
    "            return (score)\n",
    "        except:\n",
    "            return('ohh okay')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Checking for Similar News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(keyword):\n",
    "    temp = []\n",
    "    keyword = re.sub('[^a-zA-Z]',' ',keyword)\n",
    "    ss=nltk.tokenize.sent_tokenize(keyword)\n",
    "    tokenized_sent=[nltk.word_tokenize(sent) for sent in ss]\n",
    "    pos_sentences=[nltk.pos_tag(sent) for sent in tokenized_sent]\n",
    "    for i in pos_sentences:\n",
    "        print(i)\n",
    "        for j in i:\n",
    "            print(j[1])\n",
    "            if j[1] == 'NN' or j[1] == 'NNP' or j[1] == 'NNPS':\n",
    "                temp.append(j[0])\n",
    "    return temp\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@anvil.server.callable\n",
    "\n",
    "def news_processing(sentence):\n",
    "    try:\n",
    "        sentence_1 = sentence\n",
    "        sentence = clean_text(sentence)\n",
    "        for word in sentence:\n",
    "            print(word)\n",
    "            keyword = '09-09-2020'\n",
    "            value = keyword + '.csv'\n",
    "            data = os.path.join(r'C:\\Data', keyword, 'data',value)\n",
    "            data=pd.read_csv(data,  encoding='cp1252')\n",
    "            data_1= data[\"NEWS\"]\n",
    "            data_2 = data['NEWS URL']\n",
    "            super_1 = []\n",
    "            super_2 = []\n",
    "            super_3 = []\n",
    "            focus_sentence = sentence_1\n",
    "            focus_sentence = \" \".join(re.findall(\"[a-zA-Z]+\", focus_sentence)) \n",
    "            res = []\n",
    "            sent_list = []\n",
    "            urls = []\n",
    "            for row,url in zip(data_1,data_2):\n",
    "                sent=sent_tokenize(row)\n",
    "                for sentence in sent:\n",
    "                    sentence = \" \".join(re.findall(\"[a-zA-Z]+\", sentence)) \n",
    "                    corpus = [focus_sentence , sentence]\n",
    "                    vectorizer = TfidfVectorizer()\n",
    "                    trsfm=vectorizer.fit_transform(corpus)\n",
    "                    result = cosine_similarity(trsfm)[0][1]\n",
    "                    res.append(result.tolist())\n",
    "                    sent_list.append(sentence)\n",
    "                    urls.append(url)\n",
    "\n",
    "            final = res.index(max(res))\n",
    "            final_sentence = sent_list[final]\n",
    "            final_url = urls[final]\n",
    "            super_1.append(final)\n",
    "            super_2.append(final_sentence)\n",
    "            super_3.append(final_url)\n",
    "\n",
    "        final = super_1.index(max(super_1))\n",
    "        final_sentence = super_2[final]\n",
    "        urls = super_3[final]\n",
    "\n",
    "\n",
    "        return ('Input News: ',sentence_1, 'Similar News', final_sentence,'Similarity Score: ', max(res),urls)\n",
    "    except:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Whatsapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_verification(msg_1):\n",
    "    \n",
    "        check_data = msg_1\n",
    "        print('Text Retrived:' ,check_data)\n",
    "        result = processing(check_data,'comment')\n",
    "        print('Result: ', result)\n",
    "        if result == 'Not Okay':\n",
    "            msg = driver.find_elements_by_class_name('i0jNr')\n",
    "            msg = msg[-1]\n",
    "            msg.click()\n",
    "            time.sleep(2)\n",
    "\n",
    "            msg= driver.find_elements_by_class_name('_3e9My')\n",
    "\n",
    "            for msg in msg:\n",
    "                print('')\n",
    "\n",
    "            msg.click()\n",
    "            time.sleep(5)\n",
    "            msg = driver.find_element_by_xpath('//*[@id=\"app\"]/div/span[4]/div')\n",
    "            msg = driver.find_element_by_xpath('//*[@id=\"app\"]/div/span[4]/div/ul')\n",
    "            msg = driver.find_element_by_xpath('//*[@id=\"app\"]/div[1]/span[4]/div/ul/div/li[1]').click()\n",
    "\n",
    "            time.sleep(5)\n",
    "            msg_box = driver.find_element_by_xpath('//*[@id=\"main\"]/footer/div[1]/div/div/div[2]/div[1]')\n",
    "            print(check_data)\n",
    "            bot = \"Sent by bot: Your following message: *\" + check_data + \"* might have a negative social impact. Hereby, you are requested to delete it.\"\n",
    "            msg_box.send_keys(bot)\n",
    "            msg_box.send_keys(Keys.ENTER)\n",
    "            msg_box.send_keys(Keys.RETURN)\n",
    "            print('Message Sent!')\n",
    "\n",
    "        else:\n",
    "            print(\"Goes Good\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_verification():\n",
    "    \n",
    "            lastmsg = driver.find_elements_by_class_name('_33Jbv')\n",
    "            lastmsg = lastmsg[-1]\n",
    "\n",
    "            #msg= driver.find_elements_by_xpath('//*[@id=\"main\"]/div[3]/div/div[2]/div[3]/div[18]/div/div[1]/div/div/div[1]')\n",
    "            msg = driver.find_elements_by_xpath(\"((//*[@id='main']/footer/preceding::div[contains(@class, 'message-in')])[last()])/div/div/div/div/div\")\n",
    "            msg = msg[-1]\n",
    "            print(msg)\n",
    "            msg.click()\n",
    "            time.sleep(4)\n",
    "\n",
    "            msg= driver.find_elements_by_class_name('_3e9My')\n",
    "\n",
    "            for msg in msg:\n",
    "                print('')\n",
    "            \n",
    "            msg.click()\n",
    "            \n",
    "            time.sleep(5)\n",
    "            msg = driver.find_element_by_xpath('//*[@id=\"app\"]/div/span[4]/div')\n",
    "            msg = driver.find_element_by_xpath('//*[@id=\"app\"]/div/span[4]/div/ul')\n",
    "            msg = driver.find_element_by_xpath('//*[@id=\"app\"]/div[1]/span[4]/div/ul/div/li[3]').click()\n",
    "            \n",
    "            time.sleep(4)\n",
    "            import os \n",
    "\n",
    "            path = r'C:\\Users\\Sakshi\\Downloads'\n",
    "\n",
    "            for root, dirs, files in os.walk(path): \n",
    "                for file in files:  \n",
    "\n",
    "                    # change the extension from '.mp3' to  \n",
    "                    # the one of your choice. \n",
    "                    if file.startswith('WhatsApp Image 2021-10-'): \n",
    "                        image = os.path.join(path,file)\n",
    "                break\n",
    "            time.sleep(4)\n",
    "            print(image)\n",
    "\n",
    "            result_1= image_processing(image)\n",
    "            result_2 = imagetext_processing(image)\n",
    "\n",
    "            if result_1 == 'Not Okay' or result_2 == 'Not Okay':\n",
    "                time.sleep(5)\n",
    "\n",
    "                '''msg= driver.find_elements_by_class_name('i0jNr')\n",
    "                msg = msg[-1]\n",
    "                msg.click()\n",
    "                time.sleep(5)\n",
    "                '''\n",
    "\n",
    "\n",
    "                #msg= driver.find_elements_by_xpath('//*[@id=\"main\"]/div[3]/div/div[2]/div[3]/div[18]/div/div[1]/div/div/div[1]')\n",
    "                msg = driver.find_elements_by_xpath(\"((//*[@id='main']/footer/preceding::div[contains(@class, 'message-in')])[last()])/div/div/div/div/div\")\n",
    "                msg = msg[-1]\n",
    "                msg.click()\n",
    "                time.sleep(2)\n",
    "                msg = driver.find_element_by_class_name('_3e9My').click()\n",
    "                msg = driver.find_element_by_xpath('//*[@id=\"app\"]/div[1]/span[4]/div/ul/div/li[1]').click()\n",
    "                msg = driver.find_element_by_xpath('//*[@id=\"app\"]/div[1]/span[4]/div/ul/div/li[1]/div[1]').click()\n",
    "                bot = \"Sent by bot: Your following message might have a negative social impact. Hereby, you are requested to delete it.\"\n",
    "                msg_box = driver.find_element_by_xpath('//*[@id=\"main\"]/footer/div[1]/div/div/div[2]/div[1]')\n",
    "                msg_box.send_keys(bot)\n",
    "                time.sleep(2)\n",
    "                msg_box.send_keys(Keys.ENTER)\n",
    "                msg_box.send_keys(Keys.RETURN)\n",
    "                print('Message Sent!')\n",
    "\n",
    "            else:\n",
    "                print(\"Goes Good\")\n",
    "\n",
    "            os.remove(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lasttext_verification():\n",
    "    time.sleep(5)\n",
    "    #lastmsg = driver.find_elements_by_class_name('_22Msk')\n",
    "    #lastmsg = lastmsg[-1]\n",
    "    msg = driver.find_elements_by_xpath(\"((//*[@id='main']/footer/preceding::div[contains(@class, 'message-in')])[last()])/div/div/div\")\n",
    "    msg = msg[-1]\n",
    "    msg_1 = msg.text\n",
    "    msg_1 = msg_1.split('\\n')\n",
    "    print(msg_1)\n",
    "    if msg_1[1] == 'Forwarded':\n",
    "        score = news_processing(msg_1[-2])\n",
    "        print(score)\n",
    "        if score[-2] < 0.5:\n",
    "            msg = driver.find_elements_by_class_name('i0jNr')\n",
    "            msg = msg[-1]\n",
    "            msg.click()\n",
    "\n",
    "            msg= driver.find_elements_by_class_name('_3e9My')\n",
    "\n",
    "            for msg in msg:\n",
    "                print('')\n",
    "\n",
    "            msg.click()\n",
    "            time.sleep(5)\n",
    "            msg = driver.find_element_by_xpath('//*[@id=\"app\"]/div/span[4]/div')\n",
    "            msg = driver.find_element_by_xpath('//*[@id=\"app\"]/div/span[4]/div/ul')\n",
    "            msg = driver.find_element_by_xpath('//*[@id=\"app\"]/div[1]/span[4]/div/ul/div/li[1]').click()\n",
    "\n",
    "            time.sleep(5)\n",
    "            msg_box = driver.find_element_by_xpath('//*[@id=\"main\"]/footer/div[1]/div/div/div[2]/div[1]')\n",
    "            bot = \"Sent by bot: You can verify the news from  \" + score[-1]\n",
    "            msg_box.send_keys(bot)\n",
    "            msg_box.send_keys(Keys.ENTER)\n",
    "            msg_box.send_keys(Keys.ENTER)\n",
    "            msg_box.send_keys(Keys.RETURN)\n",
    "            time.sleep(8)\n",
    "            print('Message Sent!')\n",
    "\n",
    "        else:\n",
    "            print(\"News Verified\")\n",
    "\n",
    "    else:\n",
    "        text_verification(msg_1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_forever_image():\n",
    "    count = 0\n",
    "    while count!=1:\n",
    "        try:\n",
    "            print(count)\n",
    "            time.sleep(35)\n",
    "            image_verification()\n",
    "            count+=1\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_forever_text():\n",
    "    count = 0\n",
    "    while count!=1:\n",
    "        try:\n",
    "            print(count)\n",
    "            time.sleep(25)\n",
    "            lasttext_verification()\n",
    "            count+=1\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhatsApp Scaper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to scrape whatsapp messages, you need to follow the following steps:\n",
    "    1. Execute the cell given below, it shall open WhatsappWeb. Use your phone to log in into your Whatsapp using the\n",
    "    QR code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(executable_path=\"chromedriver.exe\")\n",
    "driver.get(\"https://web.whatsapp.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Once logged in execute the cell given below, but make sure that you enter the name of the whatsapp group you wish to            monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scan..\n",
      "Logged In..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Scan..\")\n",
    "print(\"Logged In..\")\n",
    "## Enter the name of the whatsapp group you wish to monitor\n",
    "names=[\"Bot\"]\n",
    "bot=\"Sent by bot: Not Okay!!\"\n",
    "time.sleep(3)\n",
    "search_box = driver.find_element_by_xpath('//*[@id=\"side\"]/div[1]/div/label/div/div[2]')\n",
    "search_box.send_keys(names)\n",
    "search_box.send_keys(Keys.ENTER)\n",
    "for i in range(1,43):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Ask any other member of the group to send a message. If the message is a simple text, or a forwarded text call the function named lasttext_verification() as shown below. If the message is an image (simple image and not a forwarded image) run the call the function named image_verification() as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Whatsapp Text Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abhishek Gupte', 'Abbe kaminey chakka hai kya tu?', '19:06']\n",
      "Text Retrived: Abbe kaminey chakka hai kya tu?\n",
      "Score:  1\n",
      "Result:  Not Okay\n",
      "\n",
      "Abbe kaminey chakka hai kya tu?\n",
      "Message Sent!\n"
     ]
    }
   ],
   "source": [
    "lasttext_verification ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Whatsapp Image Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<selenium.webdriver.remote.webelement.WebElement (session=\"167f921858f7b73661899f1458a9dfaa\", element=\"928c0ae4-f92d-4213-8d24-e402a0c41671\")>\n",
      "\n",
      "C:\\Users\\Sakshi\\Downloads\\WhatsApp Image 2021-10-03 at 19.17.40.jpeg\n",
      "jpeg\n",
      "yes\n",
      "<class 'PIL.Image.Image'>\n",
      "<class 'numpy.ndarray'>\n",
      "[[0. 0. 1. 0. 0. 0.]]\n",
      "Result: Not Okay\n",
      "Text \n",
      "Score 0\n",
      "Message Sent!\n"
     ]
    }
   ],
   "source": [
    "image_verification() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Whatsapp Forwarded Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abhishek Gupte', 'Forwarded', 'According to the chief minister of Uttar Pradesh Ayodhya airport will be named after Lord Ram', '19:07']\n",
      "[('According', 'VBG'), ('to', 'TO'), ('the', 'DT'), ('chief', 'JJ'), ('minister', 'NN'), ('of', 'IN'), ('Uttar', 'NNP'), ('Pradesh', 'NNP'), ('Ayodhya', 'NNP'), ('airport', 'NN'), ('will', 'MD'), ('be', 'VB'), ('named', 'VBN'), ('after', 'IN'), ('Lord', 'NNP'), ('Ram', 'NNP')]\n",
      "VBG\n",
      "TO\n",
      "DT\n",
      "JJ\n",
      "NN\n",
      "IN\n",
      "NNP\n",
      "NNP\n",
      "NNP\n",
      "NN\n",
      "MD\n",
      "VB\n",
      "VBN\n",
      "IN\n",
      "NNP\n",
      "NNP\n",
      "minister\n",
      "Uttar\n",
      "Pradesh\n",
      "Ayodhya\n",
      "airport\n",
      "Lord\n",
      "Ram\n",
      "('Input News: ', 'According to the chief minister of Uttar Pradesh Ayodhya airport will be named after Lord Ram', 'Similar News', 'Ayodhya The proposed airport in Ayodhya will be named after Lord Ram and will have an international status', 'Similarity Score: ', 0.42880811806142904, 'http://zeenews.india.com/uttar-pradesh/ayodhya-airport-to-be-named-after-lord-ram-will-have-international-status-2308612.html')\n",
      "\n",
      "\n",
      "Message Sent!\n"
     ]
    }
   ],
   "source": [
    "lasttext_verification ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SakshiEnv] *",
   "language": "python",
   "name": "conda-env-SakshiEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
