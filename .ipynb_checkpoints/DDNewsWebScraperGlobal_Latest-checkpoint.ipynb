{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DD News Website Scraping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm uses Beautiful Soup to scrape news related to keywords extracted from the news posted by the user and  generates a csv file containing the scraped news. This csv file acts as a dataset used to determine whether the posted news is fake or real. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note: For testing the Web App, execute the cells in Anvil Integration section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anvil Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to wss://anvil.works/uplink\n",
      "Anvil websocket open\n",
      "Authenticated OK\n"
     ]
    }
   ],
   "source": [
    "import anvil.server\n",
    "import anvil.media\n",
    "from anvil.tables import app_tables\n",
    "\n",
    "anvil.server.connect(\"FGW3TSCTAEN5R5ELCPWJEDYI-UYDDU6T4CHTAIJCC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Sakshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sakshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sakshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import csv\n",
    "import  pandas as pd\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from pytesseract import *\n",
    "#Paste your path to tesseract.exe below\n",
    "pytesseract.tesseract_cmd= r'C:\\Users\\Sakshi\\AppData\\Local\\Tesseract-OCR\\tesseract.exe'\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_url = \"http://ddnews.gov.in/national/last-rites-kansas-shooting-victim-srinivas-performed\"\n",
    "archive_url = \"http://ddnews.gov.in/about/news-archive?news_type=6\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list = []\n",
    "news_url_list = []\n",
    "page_url = []\n",
    "global page_count\n",
    "global latest_date\n",
    "global stop_date\n",
    "global scrape_day_count\n",
    "global news_date\n",
    "global news_url\n",
    "global news_title\n",
    "news_date = 2\n",
    "news_title = 1\n",
    "news_url = 0\n",
    "global while_break\n",
    "global keyword\n",
    "print_2 = ''\n",
    "print_3 = ''\n",
    "\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Exists\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('Data'):\n",
    "    print('Directory Exists')\n",
    "else:\n",
    "    os.mkdir('Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Functions for Extracting News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getandparseurl(url):\n",
    "    result = requests.get(url)\n",
    "    soup = BeautifulSoup(result.text, 'html.parser')\n",
    "    return(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmainnews(main_url):\n",
    "    soup = getandparseurl(main_url)\n",
    "    result = requests.get(main_url)\n",
    "    status_code = str(result.status_code)\n",
    "\n",
    "    raw_news = str(soup.find(\"div\", class_=\"news_content\"))\n",
    "    raw_news = (raw_news.replace('<div class=\"news_content\"><p class=\"heading_small\">', ''))\n",
    "    raw_news = (raw_news.replace('<p>', ''))\n",
    "    raw_news = (raw_news.replace('</div>', ''))\n",
    "\n",
    "\n",
    "\n",
    "    news = []\n",
    "    flag = 0\n",
    "    for i in range(0, len(raw_news)):\n",
    "        paragraph = []\n",
    "        if raw_news[i:i+4] == '</p>':\n",
    "            paragraph = raw_news[flag:i]\n",
    "            news.append(paragraph)\n",
    "            flag = i+4\n",
    "\n",
    "    news_list.append(news)\n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmainnewsfromlatest(scrape_day_count):\n",
    "    scrape_day_count = scrape_day_count\n",
    "    soup = getandparseurl(archive_url)\n",
    "    main_div = soup.find(\"div\", class_=\"main_container\")\n",
    "    view = main_div.find(\"div\", class_=\"view\")\n",
    "    item_list = list(view.findAll(\"div\", class_=\"item-list\"))\n",
    "    counter_div = item_list[1]\n",
    "    news_list_div = item_list[0]\n",
    "    counter_ul = counter_div.find(\"ul\")\n",
    "    last_page_url_raw = counter_ul.find(\"li\", class_=\"pager-last\").a.get('href', default=\"None\")\n",
    "    last_page_url = \"http://ddnews.gov.in\" + str(counter_ul.find(\"li\", class_=\"pager-last\").a.get('href', default=\"None\"))\n",
    "    i = len(last_page_url) - 1\n",
    "    page_count = \"\"\n",
    "\n",
    "    while last_page_url[i] != \"=\":\n",
    "        page_count = page_count + last_page_url[i]\n",
    "        i -= 1\n",
    "\n",
    "    page_count = int(page_count[::-1])\n",
    "\n",
    "\n",
    "    current_page_url = archive_url\n",
    "    page_url.append(current_page_url)\n",
    "    news_ul = news_list_div.find(\"ul\")\n",
    "    li_text_raw = news_ul.findAll(\"div\", class_=\"views-field views-field-nid views-field-changed\")\n",
    "\n",
    "\n",
    "    latest_date_div = li_text_raw[0].find(\"span\", class_=\"field-content\").a\n",
    "    latest_date_text = str(latest_date_div.find(\"p\", class_=\"archive-date\").text)\n",
    "    dd, mm, yyyy = map(int, latest_date_text[1:11].split('-'))\n",
    "    latest_date_raw = datetime.date(yyyy, mm, dd)\n",
    "    latest_date = latest_date_raw.strftime(\"%d-%m-%y\")\n",
    "    print_2 = \"Latest News Date: \" + latest_date\n",
    "    print(\"Latest News Date: \" + latest_date)\n",
    "\n",
    "\n",
    "\n",
    "    stop_date_raw = latest_date_raw - datetime.timedelta(days=(scrape_day_count+1)) #VALUE OF DAYS TO SUBTRACT WAS CHANGED. NOW ITS CORRECT.\n",
    "\n",
    "    stop_date = stop_date_raw.strftime(\"%d-%m-%y\")\n",
    "    print(\"Extracting all news published after: \" + stop_date)\n",
    "    print_3 = \"Extracted news after: \" + stop_date\n",
    "\n",
    "\n",
    "\n",
    "    first_sub_arr = []\n",
    "    first_sub_arr.append(\"http://ddnews.gov.in\" + str(li_text_raw[0].find(\"span\", class_=\"field-content\").a.get('href')))\n",
    "\n",
    "    title_div = li_text_raw[0].find(\"span\", class_=\"field-content\").a\n",
    "    title_text = str(title_div.find(\"p\", class_=\"archive-title\").text)\n",
    "    first_sub_arr.append(title_text)\n",
    "\n",
    "    date_div = li_text_raw[0].find(\"span\", class_=\"field-content\").a\n",
    "    date_text = str(date_div.find(\"p\", class_=\"archive-date\").text)\n",
    "    first_sub_arr.append(date_text[1:11])\n",
    "\n",
    "    news_url_list.append(first_sub_arr)\n",
    "\n",
    "\n",
    "    for i in range(1, len(li_text_raw)):\n",
    "\n",
    "        \n",
    "\n",
    "        day, month, year = map(int, news_url_list[len(news_url_list) - 1][news_date].split(\"-\"))\n",
    "        current_date_raw = datetime.date(year, month, day)\n",
    "\n",
    "        if current_date_raw <= stop_date_raw:\n",
    "\n",
    "            break\n",
    "        else:\n",
    "            sub_arr = []\n",
    "\n",
    "            sub_arr.append(\"http://ddnews.gov.in\" + str(li_text_raw[i].find(\"span\", class_=\"field-content\").a.get('href')))\n",
    "\n",
    "            title_div = li_text_raw[i].find(\"span\", class_=\"field-content\").a\n",
    "            title_text = str(title_div.find(\"p\", class_=\"archive-title\").text)\n",
    "            sub_arr.append(title_text)\n",
    "\n",
    "            date_div = li_text_raw[i].find(\"span\", class_=\"field-content\").a\n",
    "            date_text = str(date_div.find(\"p\", class_=\"archive-date\").text)\n",
    "            sub_arr.append(date_text[1:11])\n",
    "\n",
    "            news_url_list.append(sub_arr)\n",
    "\n",
    "    news_url_list.pop(len(news_url_list)-1)\n",
    "\n",
    "\n",
    "\n",
    "    status = 0\n",
    "\n",
    "    while getnextpageurl(current_page_url) != last_page_url:\n",
    "        page_url.append(getnextpageurl(current_page_url))\n",
    "        status = getnewsurl(getnextpageurl(current_page_url), stop_date_raw)\n",
    "        if status == 1:\n",
    "            break\n",
    "        current_page_url = getnextpageurl(current_page_url)\n",
    "\n",
    "    if status == 1:\n",
    "        \"\"\"print(\"Okay\")\"\"\"\n",
    "    else:\n",
    "        page_url.append(getnextpageurl(current_page_url))\n",
    "        getnewsurl(getnextpageurl(current_page_url), stop_date_raw)\n",
    "        current_page_url = getnextpageurl(current_page_url)\n",
    "    return (print_2, print_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getnextpageurl(current_page_url):\n",
    "    soup = getandparseurl(current_page_url)\n",
    "    \n",
    "\n",
    "    main_div = soup.find(\"div\", class_=\"main_container\")\n",
    "    view = main_div.find(\"div\", class_=\"view\")\n",
    "    item_list = list(view.findAll(\"div\", class_=\"item-list\"))\n",
    "    counter_div = item_list[1]\n",
    "    ul = counter_div.find(\"ul\")\n",
    "    next_page_url = \"http://ddnews.gov.in\" + str(ul.find(\"li\", class_=\"pager-next\").a.get('href', default=\"None\"))\n",
    "    \n",
    "    return next_page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getnewsurl(current_page_url, local_stop_date_raw):\n",
    "    status = 0\n",
    "    \n",
    "    soup = getandparseurl(current_page_url)\n",
    "    \n",
    "\n",
    "    main_div = soup.find(\"div\", class_=\"main_container\")\n",
    "    view = main_div.find(\"div\", class_=\"view\")\n",
    "    item_list = list(view.findAll(\"div\", class_=\"item-list\"))\n",
    "    news_list_div = item_list[0]\n",
    "\n",
    "    current_page_url = archive_url\n",
    "    page_url.append(current_page_url)\n",
    "    news_ul = news_list_div.find(\"ul\")\n",
    "    li_text_raw = news_ul.findAll(\"div\", class_=\"views-field views-field-nid views-field-changed\")\n",
    "\n",
    "    for i in range(0, len(li_text_raw)):\n",
    "\n",
    "        date_div = li_text_raw[i].find(\"span\", class_=\"field-content\").a\n",
    "        date_text = str(date_div.find(\"p\", class_=\"archive-date\").text)\n",
    "        current_date = str(date_text[1:11])\n",
    "        dd, mm, yyyy = map(int, current_date.split(\"-\"))\n",
    "        current_date_raw = datetime.date(yyyy, mm, dd)\n",
    "\n",
    "        if current_date_raw <= local_stop_date_raw:\n",
    "            #print(\"Dead end\")\n",
    "            status = 1\n",
    "            break\n",
    "        else:\n",
    "            sub_arr = []\n",
    "            sub_arr.append(\"http://ddnews.gov.in\" + str(li_text_raw[i].find(\"span\", class_=\"field-content\").a.get('href')))\n",
    "\n",
    "            title_div = li_text_raw[i].find(\"span\", class_=\"field-content\").a\n",
    "            title_text = str(title_div.find(\"p\", class_=\"archive-title\").text)\n",
    "            sub_arr.append(title_text)\n",
    "\n",
    "            sub_arr.append(date_text[1:11])\n",
    "\n",
    "            news_url_list.append(sub_arr)\n",
    "\n",
    "\n",
    "    return status\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(keyword):\n",
    "    temp = []\n",
    "    keyword = re.sub('[^a-zA-Z]',' ',keyword)\n",
    "    ss=nltk.tokenize.sent_tokenize(keyword)\n",
    "    tokenized_sent=[nltk.word_tokenize(sent) for sent in ss]\n",
    "    pos_sentences=[nltk.pos_tag(sent) for sent in tokenized_sent]\n",
    "    for i in pos_sentences:\n",
    "        print(i)\n",
    "        for j in i:\n",
    "            print(j[1])\n",
    "            if j[1] == 'NNP':\n",
    "                temp.append(j[0])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Definition and Creating csv File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Change the path wherever required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "\n",
    "def web_scraping(keyword_data, days):\n",
    "    news_sentence = keyword_data\n",
    "    scrape_day_count = 0\n",
    "    scrape_day_count = int(days)\n",
    "    sentence = keyword_data\n",
    "    print(sentence)\n",
    "    print_data = getmainnewsfromlatest(scrape_day_count)\n",
    "    print_2 = print_data[0]\n",
    "    print_3 = print_data[1]\n",
    "    print_1 = \"news extracted: \" + str(len(news_url_list))\n",
    "    print(str(len(news_url_list)) + \" news extracted:\")\n",
    "    sentence = clean_text(sentence)\n",
    "    se = sentence\n",
    "    key = keyword_data\n",
    "    for word in sentence:\n",
    "        keyword_filtered_list = [['NEWS URL', 'NEWS TITLE', 'NEWS DATE', 'NEWS']]\n",
    "        keyword = word\n",
    "        i = 0\n",
    "        path = r'C:\\Data'\n",
    "        #path = path + '\\Data'\n",
    "\n",
    "        if os.path.exists(path):\n",
    "            print('Directory Exists')\n",
    "        else:\n",
    "            os.mkdir(path)\n",
    "\n",
    "        path = os.path.join(path, keyword)\n",
    "        print(path)\n",
    "        if os.path.exists(path):\n",
    "            print('Directory Exists')\n",
    "        else:\n",
    "            os.mkdir(path)\n",
    "\n",
    "        data = path + '\\data'\n",
    "        if os.path.exists(data):\n",
    "            print('Directory Exists')\n",
    "        else:\n",
    "            os.mkdir(data)\n",
    "        #getnewsimg(keyword, i)\n",
    "\n",
    "        for i in range(len(news_url_list)):\n",
    "            sub_arr = []\n",
    "            if keyword.casefold() in str(news_url_list[i][news_title]).casefold():\n",
    "                if news_url_list[i][news_url] in keyword_filtered_list:\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\".\")\n",
    "                    sub_arr.append(news_url_list[i][news_url])\n",
    "                    sub_arr.append(news_url_list[i][news_title])\n",
    "                    sub_arr.append(str(news_url_list[i][news_date]))\n",
    "                    sub_arr.append(getmainnews(news_url_list[i][news_url]))\n",
    "                    keyword_filtered_list.append(sub_arr)\n",
    "                    #getnewsimg(keyword, i, news_url_list[i][0])\n",
    "            else:\n",
    "                print(\"..\")\n",
    "                news = getmainnews(news_url_list[i][news_url])\n",
    "                if news_url_list[i][news_url] in keyword_filtered_list:\n",
    "                    print(\"YES\")\n",
    "                    continue\n",
    "                else:\n",
    "                    for j in range(len(news)):\n",
    "                        if keyword.casefold() in news[j].casefold():\n",
    "                            if news_url_list[i][news_url] in keyword_filtered_list:\n",
    "                                print(\"YES\")\n",
    "                                break\n",
    "                            else:\n",
    "                                print(\"...\")\n",
    "                                sub_arr.append(news_url_list[i][news_url])\n",
    "                                sub_arr.append(news_url_list[i][news_title])\n",
    "                                sub_arr.append(str(news_url_list[i][news_date]))\n",
    "                                sub_arr.append(news)\n",
    "                                keyword_filtered_list.append(sub_arr)\n",
    "                                #getnewsimg(keyword, i, news_url_list[i][0])\n",
    "                                break\n",
    "\n",
    "\n",
    "        new_path = os.path.join(r'C:\\Data', keyword, 'data')\n",
    "        print(new_path)\n",
    "        os.chdir(new_path)\n",
    "        csv_name = keyword + '.csv'\n",
    "        with open(csv_name, 'w') as file:\n",
    "            writer = csv.writer(file)\n",
    "            for i in range(0, len(keyword_filtered_list)):\n",
    "                writer.writerow(keyword_filtered_list[i])\n",
    "\n",
    "\n",
    "\n",
    "        read_csv = pd.read_csv(csv_name, encoding='cp1252')\n",
    "        xlsx_name = keyword + '.xlsx'\n",
    "        read_csv.to_excel(xlsx_name, index=None, header=True)\n",
    "\n",
    "        file.close()\n",
    "\n",
    "    print(\"DATA EXTRACTED SUCCESSFULLY\")\n",
    "    return (se, print_2, print_3, print_1, \"DATA EXTRACTED SUCCESSFULLY\",key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "\n",
    "def store_data(keyword_data):\n",
    "    app_tables.table_0.delete_all_rows()\n",
    "    se = keyword_data\n",
    "    sentence_1 = clean_text(keyword_data)\n",
    "    for word in sentence_1:\n",
    "        keyword = word\n",
    "        value = keyword + '.csv'\n",
    "        data = os.path.join(r'C:\\Data', keyword, 'data',value)\n",
    "        df=pd.read_csv(data,  encoding='cp1252')\n",
    "\n",
    "        for d in df.to_dict(orient=\"records\"):\n",
    "              # d is now a dict of {columnname -> value} for this row\n",
    "              # We use Python's **kwargs syntax to pass the whole dict as\n",
    "              # keyword arguments\n",
    "            app_tables.table_0.add_row(**d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "def fetch_data():\n",
    "      return (app_tables.table_0.search())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "\n",
    "def news_processing(sentence):\n",
    "    try:\n",
    "        sentence_1 = sentence\n",
    "        sentence = clean_text(sentence)\n",
    "        for word in sentence:\n",
    "            print(word)\n",
    "            keyword = word\n",
    "            value = keyword + '.csv'\n",
    "            data = os.path.join(r'C:\\Data', keyword, 'data',value)\n",
    "            print(data)\n",
    "            data=pd.read_csv(data,  encoding='cp1252')\n",
    "            data_1= data[\"NEWS\"]\n",
    "            data_2 = data['NEWS URL']\n",
    "            super_1 = []\n",
    "            super_2 = []\n",
    "            super_3 = []\n",
    "            focus_sentence = sentence_1\n",
    "            focus_sentence = \" \".join(re.findall(\"[a-zA-Z]+\", focus_sentence)) \n",
    "            res = []\n",
    "            sent_list = []\n",
    "            urls = []\n",
    "            for row,url in zip(data_1,data_2):\n",
    "                sent=sent_tokenize(row)\n",
    "                for sentence in sent:\n",
    "                    sentence = \" \".join(re.findall(\"[a-zA-Z]+\", sentence)) \n",
    "                    corpus = [focus_sentence , sentence]\n",
    "                    vectorizer = TfidfVectorizer()\n",
    "                    trsfm=vectorizer.fit_transform(corpus)\n",
    "                    result = cosine_similarity(trsfm)[0][1]\n",
    "                    res.append(result.tolist())\n",
    "                    sent_list.append(sentence)\n",
    "                    urls.append(url)\n",
    "            print('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX')\n",
    "            final = res.index(max(res))\n",
    "            final_sentence = sent_list[final]\n",
    "            final_url = urls[final]\n",
    "            super_1.append(final)\n",
    "            super_2.append(final_sentence)\n",
    "            super_3.append(final_url)\n",
    "            print(final_sentence, max(res))\n",
    "        final = super_1.index(max(super_1))\n",
    "        final_sentence = super_2[final]\n",
    "        urls = super_3[final]\n",
    "\n",
    "\n",
    "        return ('Input News: ',sentence_1, 'Similar News', final_sentence,'Similarity Score: ', max(res),urls)\n",
    "    except:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@anvil.server.callable\n",
    "\n",
    "def img_text(path, days):\n",
    "    with anvil.media.TempFile(path) as filename:\n",
    "        img=Image.open(filename)\n",
    "        text=pytesseract.image_to_string(img)\n",
    "        result = web_scraping(text,days)\n",
    "        #result = news_processing(text)\n",
    "        return (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Anvil Integration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass any news or keyword that you want to scrape from DD News website and the number of days you want to scrape from in the function given below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIA special court sentences fifteen accused convicted in ISIS Conspiracy Delhi Case\n",
      "Latest News Date: 17-10-20\n",
      "Extracting all news published after: 16-10-20\n",
      "12 news extracted:\n",
      "[('NIA', 'NNP'), ('special', 'JJ'), ('court', 'NN'), ('sentences', 'NNS'), ('fifteen', 'VBP'), ('accused', 'VBN'), ('convicted', 'VBN'), ('in', 'IN'), ('ISIS', 'NNP'), ('Conspiracy', 'NNP'), ('Delhi', 'NNP'), ('Case', 'NNP')]\n",
      "NNP\n",
      "JJ\n",
      "NN\n",
      "NNS\n",
      "VBP\n",
      "VBN\n",
      "VBN\n",
      "IN\n",
      "NNP\n",
      "NNP\n",
      "NNP\n",
      "NNP\n",
      "Directory Exists\n",
      "C:\\Data\\NIA\n",
      ".\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      ".\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "C:\\Data\\NIA\\data\n",
      "Directory Exists\n",
      "C:\\Data\\ISIS\n",
      ".\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      ".\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "C:\\Data\\ISIS\\data\n",
      "Directory Exists\n",
      "C:\\Data\\Conspiracy\n",
      ".\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      ".\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "C:\\Data\\Conspiracy\\data\n",
      "Directory Exists\n",
      "C:\\Data\\Delhi\n",
      ".\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "...\n",
      "..\n",
      ".\n",
      "..\n",
      "..\n",
      "..\n",
      "..\n",
      "...\n",
      "..\n",
      "C:\\Data\\Delhi\\data\n",
      "Directory Exists\n",
      "C:\\Data\\Case\n",
      ".\n",
      "..\n",
      "..\n",
      ".\n",
      "..\n",
      "..\n",
      ".\n",
      "..\n",
      "..\n",
      ".\n",
      "..\n",
      "..\n",
      "C:\\Data\\Case\\data\n",
      "DATA EXTRACTED SUCCESSFULLY\n"
     ]
    }
   ],
   "source": [
    "result = web_scraping('NIA special court sentences fifteen accused convicted in ISIS Conspiracy Delhi Case', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the news is scraped and the csv file is created, you can check for the contents of the csv file from the Directory you have created it in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, formulate a news/tweet/post by referring to the csv file created and pass it to the function as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NIA', 'NNP'), ('special', 'JJ'), ('court', 'NN'), ('sentences', 'NNS'), ('fifteen', 'VBP'), ('accused', 'VBN'), ('convicted', 'VBN'), ('in', 'IN'), ('ISIS', 'NNP'), ('Conspiracy', 'NNP'), ('Delhi', 'NNP'), ('Case', 'NNP')]\n",
      "NNP\n",
      "JJ\n",
      "NN\n",
      "NNS\n",
      "VBP\n",
      "VBN\n",
      "VBN\n",
      "IN\n",
      "NNP\n",
      "NNP\n",
      "NNP\n",
      "NNP\n",
      "NIA\n",
      "C:\\Data\\NIA\\data\\NIA.csv\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "In an order passed on Parveen Singh Special Judge for NIA at Patiala House Courts Complex New Delhi pronounced sentence on accused persons convicted in ISIS Conspiracy Delhi Case This case was registered at PS NIA New Delhi on under section of IPC and sections B amp of UA P Act and pertains to xa criminal conspiracy hatched by the ISIS to establish its base in India by recruiting Muslim youth for ISIS a proscribed terrorist organization by using different social media platforms 0.3200791050744479\n",
      "ISIS\n",
      "C:\\Data\\ISIS\\data\\ISIS.csv\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "In an order passed on Parveen Singh Special Judge for NIA at Patiala House Courts Complex New Delhi pronounced sentence on accused persons convicted in ISIS Conspiracy Delhi Case This case was registered at PS NIA New Delhi on under section of IPC and sections B amp of UA P Act and pertains to xa criminal conspiracy hatched by the ISIS to establish its base in India by recruiting Muslim youth for ISIS a proscribed terrorist organization by using different social media platforms 0.3200791050744479\n",
      "Conspiracy\n",
      "C:\\Data\\Conspiracy\\data\\Conspiracy.csv\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "In an order passed on Parveen Singh Special Judge for NIA at Patiala House Courts Complex New Delhi pronounced sentence on accused persons convicted in ISIS Conspiracy Delhi Case This case was registered at PS NIA New Delhi on under section of IPC and sections B amp of UA P Act and pertains to xa criminal conspiracy hatched by the ISIS to establish its base in India by recruiting Muslim youth for ISIS a proscribed terrorist organization by using different social media platforms 0.3200791050744479\n",
      "Delhi\n",
      "C:\\Data\\Delhi\\data\\Delhi.csv\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "In an order passed on Parveen Singh Special Judge for NIA at Patiala House Courts Complex New Delhi pronounced sentence on accused persons convicted in ISIS Conspiracy Delhi Case This case was registered at PS NIA New Delhi on under section of IPC and sections B amp of UA P Act and pertains to xa criminal conspiracy hatched by the ISIS to establish its base in India by recruiting Muslim youth for ISIS a proscribed terrorist organization by using different social media platforms 0.3200791050744479\n",
      "Case\n",
      "C:\\Data\\Case\\data\\Case.csv\n",
      "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "In an order passed on Parveen Singh Special Judge for NIA at Patiala House Courts Complex New Delhi pronounced sentence on accused persons convicted in ISIS Conspiracy Delhi Case This case was registered at PS NIA New Delhi on under section of IPC and sections B amp of UA P Act and pertains to xa criminal conspiracy hatched by the ISIS to establish its base in India by recruiting Muslim youth for ISIS a proscribed terrorist organization by using different social media platforms 0.3200791050744479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Input News: ',\n",
       " 'NIA special court sentences fifteen accused convicted in ISIS Conspiracy Delhi Case',\n",
       " 'Similar News',\n",
       " 'In an order passed on Parveen Singh Special Judge for NIA at Patiala House Courts Complex New Delhi pronounced sentence on accused persons convicted in ISIS Conspiracy Delhi Case This case was registered at PS NIA New Delhi on under section of IPC and sections B amp of UA P Act and pertains to xa criminal conspiracy hatched by the ISIS to establish its base in India by recruiting Muslim youth for ISIS a proscribed terrorist organization by using different social media platforms',\n",
       " 'Similarity Score: ',\n",
       " 0.3200791050744479,\n",
       " 'http://ddnews.gov.in/national/nia-special-court-sentences-fifteen-accused-convicted-isis-conspiracy-delhi-case')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_processing('NIA special court sentences fifteen accused convicted in ISIS Conspiracy Delhi Case')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
